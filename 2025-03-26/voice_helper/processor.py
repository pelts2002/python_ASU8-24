# venv\Scripts\activate

import os
import wave
import pyaudio
import sys
import webbrowser
import nltk
from nltk.tokenize import word_tokenize
import pymorphy3
from sympy.strategies.core import switch

nltk.download('punkt')  # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
morph = pymorphy3.MorphAnalyzer()

RESPONSES = {
    "–ø—Ä–∏–≤–µ—Ç": "audioFiles/hello.wav",
    "–∫–∞–∫ –¥–µ–ª–∞": "audioFiles/aboutme.wav",
    "–ø–æ–∫–∞": "audioFiles/goodbye.wav"
}


def play_audio(filename):
    """–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª"""
    if not os.path.exists(filename):
        print(f"‚ö†Ô∏è –§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    chunk = 1024
    wf = wave.open(filename, "rb")
    pa = pyaudio.PyAudio()
    stream = pa.open(format=pa.get_format_from_width(wf.getsampwidth()),
                     channels=wf.getnchannels(),
                     rate=wf.getframerate(),
                     output=True)

    data = wf.readframes(chunk)
    while data:
        stream.write(data)
        data = wf.readframes(chunk)

    stream.stop_stream()
    stream.close()
    pa.terminate()
    wf.close()

    print(f"üîä –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥—ë–Ω —Ñ–∞–π–ª: {filename}")


def check_patterns(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∞—É–¥–∏–æ—Ñ–∞–π–ª"""
    for pattern, audio_file in RESPONSES.items():
        if pattern in text.lower():  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ä–µ–≥–∏—Å—Ç—Ä
            print(f"üîç –ù–∞–π–¥–µ–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: '{pattern}'")
            play_audio(audio_file)

def open_weather(text):
    """–û—Ç–∫—Ä—ã–≤–∞–µ—Ç –±—Ä–∞—É–∑–µ—Ä —Å –ø–æ–∏—Å–∫–æ–º '–ü–æ–≥–æ–¥–∞ –≤ <–≥–æ—Ä–æ–¥>'"""
    words = text.lower().split()
    
    if len(words) >= 3 and words[0] == "–ø–æ–≥–æ–¥–∞" and words[1] == "–≤":
        city = " ".join(words[2:])  # –ë–µ—Ä—ë–º –≤—Å—ë –ø–æ—Å–ª–µ "–ø–æ–≥–æ–¥–∞ –≤"
        print(f"üå§ –ò—â—É –ø–æ–≥–æ–¥—É –¥–ª—è: {city}")
        url = f"https://www.google.com/search?q=–ü–æ–≥–æ–¥–∞+–≤+{city}"
        webbrowser.open(url)
        return True  # –ö–æ–º–∞–Ω–¥–∞ –Ω–∞–π–¥–µ–Ω–∞
    
    return False  # –ö–æ–º–∞–Ω–¥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞

def process_text(text):
    print(f"üìå –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç: {text}")

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = word_tokenize(text)
    print(f"üîπ –¢–æ–∫–µ–Ω—ã: {tokens}")

    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    lemmas = [morph.parse(word)[0].normal_form for word in tokens]
    lemmatized_text = " ".join(lemmas)
    print(f"üîπ –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {lemmatized_text}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ—Ç–∫—Ä—ã—Ç—å –ø–æ–≥–æ–¥—É
    if open_weather(lemmatized_text):  
        return  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–æ–º–∞–Ω–¥—É –¥–ª—è –ø–æ–≥–æ–¥—ã, –¥–∞–ª—å—à–µ –Ω–µ –∏–¥—ë–º

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—É–¥–∏–æ–∫–æ–º–∞–Ω–¥—ã
    check_patterns(lemmatized_text)  

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    with open("recognized_text.txt", "a", encoding="utf-8") as file:
        file.write(lemmatized_text + "\n")

    print("‚úÖ –¢–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ recognized_text.txt")